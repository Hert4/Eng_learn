{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-25T17:07:11.726118Z",
     "iopub.status.busy": "2025-05-25T17:07:11.725836Z",
     "iopub.status.idle": "2025-05-25T17:08:46.114313Z",
     "shell.execute_reply": "2025-05-25T17:08:46.113475Z",
     "shell.execute_reply.started": "2025-05-25T17:07:11.726097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install fastapi uvicorn onnxruntime torch soundfile numpy aiofiles starlette\n",
    "!pip install pyngrok\n",
    "!git clone https://github.com/SparkAudio/Spark-TTS\n",
    "!pip install omegaconf einx\n",
    "!pip install aiortc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:08:46.116107Z",
     "iopub.status.busy": "2025-05-25T17:08:46.115752Z",
     "iopub.status.idle": "2025-05-25T17:08:47.099785Z",
     "shell.execute_reply": "2025-05-25T17:08:47.099279Z",
     "shell.execute_reply.started": "2025-05-25T17:08:46.116082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Gắn ngrok token của bạn (chạy 1 lần)\n",
    "ngrok.set_auth_token(\"2n6gGjwozlbdtvvfyCgJZOJO1o3_jqxBHhEAAnqviQNL9jiQ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:13:05.574358Z",
     "iopub.status.busy": "2025-05-25T17:13:05.573777Z",
     "iopub.status.idle": "2025-05-25T17:13:05.583626Z",
     "shell.execute_reply": "2025-05-25T17:13:05.583036Z",
     "shell.execute_reply.started": "2025-05-25T17:13:05.574332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import onnxruntime as ort\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor\n",
    "# import torch\n",
    "# import string\n",
    "# import io\n",
    "# import soundfile as sf\n",
    "# import base64\n",
    "# from fastapi import FastAPI, WebSocket\n",
    "# from starlette.websockets import WebSocketState\n",
    "# import json\n",
    "# import re\n",
    "# import asyncio\n",
    "# import sys\n",
    "# import logging\n",
    "# from huggingface_hub import snapshot_download\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "# # Logging setup\n",
    "# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Import Spark-TTS\n",
    "# sys.path.append(\"Spark-TTS\")\n",
    "# from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
    "\n",
    "# # FastAPI app\n",
    "# app = FastAPI()\n",
    "\n",
    "# # Enable CORS for all origins (adjust as needed for production)\n",
    "# app.add_middleware(\n",
    "#     CORSMiddleware,\n",
    "#     allow_origins=[\"*\"],  \n",
    "#     allow_credentials=True,\n",
    "#     allow_methods=[\"*\"],\n",
    "#     allow_headers=[\"*\"],\n",
    "# )\n",
    "\n",
    "# # Constants\n",
    "# HG_MODEL = \"livekit/turn-detector\"\n",
    "# ONNX_FILENAME = \"/kaggle/input/model_2/onnx/default/1/model_quantized.onnx\"\n",
    "# EOU_THRESHOLD = 0.5\n",
    "# MAX_HISTORY = 2\n",
    "# MAX_HISTORY_TOKENS = 512\n",
    "# PUNCS = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "# # Load models\n",
    "# turn_tokenizer = AutoTokenizer.from_pretrained(HG_MODEL)\n",
    "# onnx_session = ort.InferenceSession(ONNX_FILENAME, providers=[\"CPUExecutionProvider\"])\n",
    "# qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "# qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# snapshot_download(\"beyoru/Spark-TTS-0.5B-with-KafkaSpark\", local_dir=\"Spark-TTS-0.5B-with-KafkaSpark\")\n",
    "# spark_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"Spark-TTS-0.5B-with-KafkaSpark/LLM\"\n",
    "# ).to(\"cuda\")\n",
    "# spark_tokenizer = AutoProcessor.from_pretrained(\"Spark-TTS-0.5B-with-KafkaSpark/LLM\")\n",
    "# audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B-with-KafkaSpark\")\n",
    "\n",
    "# # Helpers\n",
    "# # Helper functions\n",
    "# def softmax(logits):\n",
    "#     try:\n",
    "#         exp_logits = np.exp(logits - np.max(logits))\n",
    "#         return exp_logits / np.sum(exp_logits)\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Softmax error: {e}\")\n",
    "#         return np.zeros_like(logits)\n",
    "\n",
    "\n",
    "# def normalize_text(text):\n",
    "#     try:\n",
    "\n",
    "#         def strip_puncs(text):\n",
    "#             return text.translate(str.maketrans(\"\", \"\", PUNCS))\n",
    "\n",
    "#         return \" \".join(strip_puncs(text).lower().split())\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Normalize text error: {e}\")\n",
    "#         return text\n",
    "\n",
    "\n",
    "# def format_chat_ctx(chat_ctx):\n",
    "#     try:\n",
    "#         new_chat_ctx = []\n",
    "#         for msg in chat_ctx:\n",
    "#             if msg[\"role\"] in (\"user\", \"assistant\"):\n",
    "#                 content = normalize_text(msg[\"content\"])\n",
    "#                 if content:\n",
    "#                     msg[\"content\"] = content\n",
    "#                     new_chat_ctx.append(msg)\n",
    "#         convo_text = turn_tokenizer.apply_chat_template(\n",
    "#             new_chat_ctx,\n",
    "#             add_generation_prompt=False,\n",
    "#             add_special_tokens=False,\n",
    "#             tokenize=False,\n",
    "#         )\n",
    "#         ix = convo_text.rfind(\"<|im_end|>\")\n",
    "#         return convo_text[:ix]\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Format chat context error: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# def calculate_eou(chat_ctx, session):\n",
    "#     try:\n",
    "#         formatted_text = format_chat_ctx(chat_ctx[-MAX_HISTORY:])\n",
    "#         inputs = turn_tokenizer(\n",
    "#             formatted_text,\n",
    "#             return_tensors=\"np\",\n",
    "#             truncation=True,\n",
    "#             max_length=MAX_HISTORY_TOKENS,\n",
    "#         )\n",
    "#         input_ids = inputs[\"input_ids\"].astype(np.int64)\n",
    "#         outputs = session.run([\"logits\"], {\"input_ids\": input_ids})\n",
    "#         logits = outputs[0][0, -1, :]\n",
    "#         probs = softmax(logits)\n",
    "#         eou_token_id = turn_tokenizer.encode(\"<|im_end|>\")[0]\n",
    "#         logger.debug(f\"Input text: {formatted_text}, EOU token ID: {eou_token_id}, Probs: {probs}\")\n",
    "#         return probs[eou_token_id]\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"EOU calculation error: {e}\")\n",
    "#         return 0.0\n",
    "\n",
    "# async def generate_speech_from_text(text: str) -> np.ndarray:\n",
    "#     try:\n",
    "#         prompt = \"\".join(\n",
    "#             [\n",
    "#                 \"<|task_tts|>\",\n",
    "#                 \"<|start_content|>\",\n",
    "#                 text,\n",
    "#                 \"<|end_content|>\",\n",
    "#                 \"<|start_global_token|>\",\n",
    "#             ]\n",
    "#         )\n",
    "#         model_inputs = spark_tokenizer([prompt], return_tensors=\"pt\").to(\n",
    "#             spark_model.device\n",
    "#         )\n",
    "#         generated_ids = spark_model.generate(\n",
    "#             **model_inputs,\n",
    "#             max_new_tokens=1024,\n",
    "#             temperature = 0.2,\n",
    "#             do_sample = True,\n",
    "#             # eos_token_id=spark_tokenizer.eos_token_id,\n",
    "#             pad_token_id=spark_tokenizer.pad_token_id,\n",
    "#         )\n",
    "#         generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1] :]\n",
    "#         predicts_text = spark_tokenizer.batch_decode(\n",
    "#             generated_ids_trimmed, skip_special_tokens=False\n",
    "#         )[0]\n",
    "#         semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicts_text)\n",
    "#         if not semantic_matches:\n",
    "#             logger.warning(\"No semantic matches found in TTS output\")\n",
    "#             return np.array([], dtype=np.float32)\n",
    "#         pred_semantic_ids = (\n",
    "#             torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0)\n",
    "#         )\n",
    "#         global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicts_text)\n",
    "#         pred_global_ids = (\n",
    "#             torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0)\n",
    "#             if global_matches\n",
    "#             else torch.zeros((1, 1), dtype=torch.long)\n",
    "#         )\n",
    "#         pred_global_ids = pred_global_ids.unsqueeze(0)\n",
    "#         wav_np = audio_tokenizer.detokenize(\n",
    "#             pred_global_ids.squeeze(0), pred_semantic_ids\n",
    "#         )\n",
    "#         return wav_np\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Speech generation error: {e}\")\n",
    "#         return np.array([], dtype=np.float32)\n",
    "\n",
    "# def waveform_to_base64(waveform: np.ndarray, sample_rate=16000) -> str:\n",
    "#     try:\n",
    "#         if waveform.size == 0: \n",
    "#             return \"\"\n",
    "#         buf = io.BytesIO()\n",
    "#         sf.write(buf, waveform, sample_rate, format=\"WAV\")\n",
    "#         return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"waveform->b64 error: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# class ConversationManager:\n",
    "#     def __init__(self):\n",
    "#         self.chat_history = []\n",
    "#         self.sample_rate = 16000\n",
    "#         self.is_processing = False\n",
    "#         self.lock = asyncio.Lock()\n",
    "#         self.transcript_buffer = []\n",
    "\n",
    "#     async def process_conversation(self, transcript: str) -> tuple:\n",
    "#         async with self.lock:\n",
    "#             if self.is_processing:\n",
    "#                 logger.info(\"Skipping transcript processing: already in progress\")\n",
    "#                 return transcript, 0.0, False, None, None\n",
    "#             self.is_processing = True\n",
    "#             try:\n",
    "#                 if not transcript.strip():\n",
    "#                     logger.debug(\"Received empty transcript, skipping\")\n",
    "#                     return transcript, 0.0, False, None, None\n",
    "\n",
    "#                 # Log input\n",
    "#                 logger.debug(f\"Received transcript: {transcript}\")\n",
    "\n",
    "#                 # Add new transcript to buffer\n",
    "#                 self.transcript_buffer.append(transcript)\n",
    "#                 logger.debug(f\"Buffered transcript: {transcript}, Buffer: {self.transcript_buffer}\")\n",
    "\n",
    "#                 # Calculate EOU with current chat history plus buffered transcripts\n",
    "#                 current_transcript = \" \".join(self.transcript_buffer)\n",
    "#                 messages = self.chat_history + [\n",
    "#                     {\"role\": \"user\", \"content\": current_transcript}\n",
    "#                 ]\n",
    "#                 eou_prob = calculate_eou(messages, onnx_session)\n",
    "#                 logger.debug(f\"EOU score for '{current_transcript}': {eou_prob}\")\n",
    "\n",
    "#                 # Set is_final based on EOU score\n",
    "#                 is_final = eou_prob >= EOU_THRESHOLD\n",
    "#                 logger.debug(f\"is_final set to {is_final} based on eou_score: {eou_prob}\")\n",
    "\n",
    "#                 # Process only if is_final is True (i.e., eou_prob >= EOU_THRESHOLD)\n",
    "#                 if not is_final:\n",
    "#                     logger.debug(f\"EOU score {eou_prob} below threshold {EOU_THRESHOLD}, buffering\")\n",
    "#                     return current_transcript, eou_prob, False, None, None\n",
    "\n",
    "#                 # Process the concatenated transcript\n",
    "#                 logger.info(f\"Processing final transcript: {current_transcript}\")\n",
    "#                 inputs = qwen_tokenizer.apply_chat_template(\n",
    "#                     messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "#                 ).to(qwen_model.device)\n",
    "#                 outputs = qwen_model.generate(inputs, max_new_tokens=156)\n",
    "#                 generated_text = qwen_tokenizer.decode(\n",
    "#                     outputs[0][inputs.shape[1]:], skip_special_tokens=True\n",
    "#                 )\n",
    "#                 logger.info(f\"Generated response: {generated_text}\")\n",
    "\n",
    "#                 # Update chat history\n",
    "#                 self.chat_history.append({\"role\": \"user\", \"content\": current_transcript})\n",
    "#                 self.chat_history.append({\"role\": \"assistant\", \"content\": generated_text})\n",
    "#                 if len(self.chat_history) > MAX_HISTORY * 2:\n",
    "#                     self.chat_history = self.chat_history[-MAX_HISTORY * 2:]\n",
    "\n",
    "#                 # Generate audio\n",
    "#                 generated_waveform = await generate_speech_from_text(generated_text)\n",
    "#                 audio_base64 = waveform_to_base64(generated_waveform, self.sample_rate)\n",
    "\n",
    "#                 # Clear buffer after processing\n",
    "#                 self.transcript_buffer = []\n",
    "#                 logger.debug(\"Cleared transcript buffer\")\n",
    "\n",
    "#                 return current_transcript, eou_prob, True, generated_text, audio_base64\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"Conversation processing error: {e}\")\n",
    "#                 return current_transcript, eou_prob, False, None, None\n",
    "#             finally:\n",
    "#                 self.is_processing = False\n",
    "\n",
    "# @app.get(\"/\")\n",
    "# async def root():\n",
    "#     return {\"message\": \"Assistant WebSocket server is running. Connect via /ws/assistant\"}\n",
    "\n",
    "# @app.get(\"/test\")\n",
    "# async def test_page():\n",
    "#     return \"\"\"\n",
    "#     <!DOCTYPE html>\n",
    "#     <html>\n",
    "#     <head><title>Assistant WS Test</title></head>\n",
    "#     <body>\n",
    "#     <h1>Assistant WebSocket Test</h1>\n",
    "#     <textarea id=\"transcript\" rows=\"4\" cols=\"50\"></textarea><br/>\n",
    "#     <button onclick=\"sendTranscript()\">Send Transcript</button>\n",
    "#     <pre id=\"output\"></pre>\n",
    "#     <script>\n",
    "#       const ws = new WebSocket(\"ws://\" + location.host + \"/ws/assistant\");\n",
    "#       ws.onmessage = (event) => {\n",
    "#         const data = JSON.parse(event.data);\n",
    "#         document.getElementById('output').textContent += JSON.stringify(data, null, 2) + \"\\\\n\";\n",
    "#       };\n",
    "#       function sendTranscript() {\n",
    "#         const transcript = document.getElementById('transcript').value;\n",
    "#         ws.send(JSON.stringify({transcript: transcript}));\n",
    "#       }\n",
    "#     </script>\n",
    "#     </body>\n",
    "#     </html>\n",
    "#     \"\"\"\n",
    "\n",
    "# @app.on_event(\"startup\")\n",
    "# async def startup_event():\n",
    "#     logger.info(\"Starting Assistant server...\")\n",
    "\n",
    "# @app.on_event(\"shutdown\")\n",
    "# async def shutdown_event():\n",
    "#     logger.info(\"Shutting down Assistant server...\")\n",
    "\n",
    "# @app.websocket(\"/ws/assistant\")\n",
    "# async def websocket_endpoint(websocket: WebSocket):\n",
    "#     await websocket.accept()\n",
    "#     cm = ConversationManager()\n",
    "#     try:\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 data = json.loads(await websocket.receive_text())\n",
    "#                 transcript = data.get(\"transcript\", \"\")\n",
    "#                 logger.info(f\"WebSocket input: transcript={transcript}\")\n",
    "#                 if not transcript.strip():\n",
    "#                     continue\n",
    "#                 # Server quyết định is_final dựa trên eou_score\n",
    "#                 transcript, eou_score, complete, resp, audio_b64 = await cm.process_conversation(transcript)\n",
    "#                 await websocket.send_json({\"type\": \"transcript\", \"transcript\": transcript, \"eou_score\": float(eou_score), \"is_complete\": complete})\n",
    "#                 if resp:\n",
    "#                     await asyncio.sleep(0.1)\n",
    "#                     await websocket.send_json({\"type\": \"assistant_response\", \"response\": resp, \"audio\": audio_b64})\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"WebSocket error: {e}\")\n",
    "#                 if websocket.client_state == WebSocketState.CONNECTED:\n",
    "#                     await websocket.close(code=1000, reason=str(e))\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"WebSocket connection closed with error: {e}\")\n",
    "        \n",
    "# # === Run with ngrok ===\n",
    "# from pyngrok import ngrok\n",
    "# public_url = ngrok.connect(8000)\n",
    "# print(\"Assistant public URL:\", public_url)\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()  # Allows nested event loops in notebooks\n",
    "\n",
    "# import uvicorn\n",
    "# from uvicorn.config import Config\n",
    "# from uvicorn.server import Server\n",
    "\n",
    "# config = Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "# server = Server(config=config)\n",
    "# await server.serve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:38.396Z",
     "iopub.execute_input": "2025-05-25T17:13:07.825283Z",
     "iopub.status.busy": "2025-05-25T17:13:07.824733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44809a2b69da4ac793c4f2165530ce2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81664a603c5348b98f3be9ccaf8b0cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b82ea320cd418b9462b413316d21c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 32 files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef988585a864bfd8e47442839e70710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f5542268d0459bb22479f6b9d81033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/509k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6107ffc03b34a3ca452c9d874e67fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58891e322b274e44a8e3e0c222a575f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/2.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70db61e83fa49669458b5c9a2ee85c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/626M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda71341ad72403886d08b024c2bbf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/170 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3daeefc93e4cefb4fa8e0b220dd217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/741 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f99dafee694c53a4a17dbc674472bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69997c5c4acb4f618bbc335d267910b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af62b60fa37d42e8aa306674a63f206d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335f43274f52404fb036c5a5746ad5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1773b0be01d4c5997a030277750c0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/169 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a3c2e8d06e463db92f51afd56728c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61cf423298248a7bb9a2c45852e0797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce6ffe5717046b4ba445e8f13cdcfb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gradio_TTS.png:   0%|          | 0.00/81.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tensor: mel_transformer.spectrogram.window\n",
      "Missing tensor: mel_transformer.mel_scale.fb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/2975910188.py:285: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "/tmp/ipykernel_35/2975910188.py:289: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"shutdown\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant public URL: NgrokTunnel: \"https://03d7-34-30-116-120.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [35]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     ('2405:4803:fe20:bb70:1c0f:707b:23f4:8799', 0) - \"WebSocket /ws/assistant\" [accepted]\n",
      "INFO:     connection open\n",
      "INFO:     ('2405:4803:fe20:bb70:1c0f:707b:23f4:8799', 0) - \"WebSocket /ws/assistant\" [accepted]\n",
      "INFO:     connection open\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor\n",
    "import torch\n",
    "import string\n",
    "import io\n",
    "import soundfile as sf\n",
    "import base64\n",
    "from fastapi import FastAPI, WebSocket\n",
    "from starlette.websockets import WebSocketState\n",
    "import json\n",
    "import re\n",
    "import asyncio\n",
    "import sys\n",
    "import logging\n",
    "from huggingface_hub import snapshot_download\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from aiortc import RTCPeerConnection, RTCSessionDescription, AudioStreamTrack\n",
    "import av\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import Spark-TTS\n",
    "sys.path.append(\"Spark-TTS\")\n",
    "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Constants\n",
    "HG_MODEL = \"livekit/turn-detector\"\n",
    "ONNX_FILENAME = \"/kaggle/input/model_2/onnx/default/1/model_quantized.onnx\"\n",
    "EOU_THRESHOLD = 0.5\n",
    "MAX_HISTORY = 2\n",
    "MAX_HISTORY_TOKENS = 512\n",
    "PUNCS = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "# Load models\n",
    "try:\n",
    "    turn_tokenizer = AutoTokenizer.from_pretrained(HG_MODEL)\n",
    "    onnx_session = ort.InferenceSession(ONNX_FILENAME, providers=[\"CPUExecutionProvider\"])\n",
    "    qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "    qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "    snapshot_download(\"beyoru/Spark-TTS-0.5B-with-KafkaSpark\", local_dir=\"Spark-TTS-0.5B-with-KafkaSpark\")\n",
    "    spark_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Spark-TTS-0.5B-with-KafkaSpark/LLM\"\n",
    "    ).to(\"cuda\")\n",
    "    spark_tokenizer = AutoProcessor.from_pretrained(\"Spark-TTS-0.5B-with-KafkaSpark/LLM\")\n",
    "    audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B-with-KafkaSpark\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Model loading error: {e}\")\n",
    "    raise\n",
    "\n",
    "# WebRTC setup\n",
    "pcs = set()  # Store active peer connections\n",
    "\n",
    "class AudioStreamTrackImpl(AudioStreamTrack):\n",
    "    kind = \"audio\"\n",
    "\n",
    "    def __init__(self, waveform, sample_rate=16000):\n",
    "        super().__init__()\n",
    "        self.waveform = waveform\n",
    "        self.sample_rate = sample_rate\n",
    "        self.pos = 0\n",
    "        self.frame_duration = 0.02  # 20ms frames\n",
    "        self.samples_per_frame = int(sample_rate * self.frame_duration)\n",
    "\n",
    "    async def recv(self):\n",
    "        if self.pos >= len(self.waveform):\n",
    "            return None\n",
    "        samples = self.waveform[self.pos:self.pos + self.samples_per_frame]\n",
    "        self.pos += self.samples_per_frame\n",
    "        if len(samples) < self.samples_per_frame:\n",
    "            samples = np.pad(samples, (0, self.samples_per_frame - len(samples)), mode='constant')\n",
    "        frame = av.AudioFrame.from_ndarray(samples.reshape(-1, 1), format=\"s16\", layout=\"mono\")\n",
    "        frame.sample_rate = self.sample_rate\n",
    "        frame.pts = int(self.pos / self.sample_rate * 90000)  # 90kHz clock\n",
    "        return frame\n",
    "\n",
    "# Helper functions (unchanged)\n",
    "def softmax(logits):\n",
    "    try:\n",
    "        exp_logits = np.exp(logits - np.max(logits))\n",
    "        return exp_logits / np.sum(exp_logits)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Softmax error: {e}\")\n",
    "        return np.zeros_like(logits)\n",
    "\n",
    "def normalize_text(text):\n",
    "    try:\n",
    "        def strip_puncs(text):\n",
    "            return text.translate(str.maketrans(\"\", \"\", PUNCS))\n",
    "        return \" \".join(strip_puncs(text).lower().split())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Normalize text error: {e}\")\n",
    "        return text\n",
    "\n",
    "def format_chat_ctx(chat_ctx):\n",
    "    try:\n",
    "        new_chat_ctx = []\n",
    "        for msg in chat_ctx:\n",
    "            if msg[\"role\"] in (\"user\", \"assistant\"):\n",
    "                content = normalize_text(msg[\"content\"])\n",
    "                if content:\n",
    "                    msg[\"content\"] = content\n",
    "                    new_chat_ctx.append(msg)\n",
    "        convo_text = turn_tokenizer.apply_chat_template(\n",
    "            new_chat_ctx,\n",
    "            add_generation_prompt=False,\n",
    "            add_special_tokens=False,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        ix = convo_text.rfind(\"<|im_end|>\")\n",
    "        return convo_text[:ix]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Format chat context error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_eou(chat_ctx, session):\n",
    "    try:\n",
    "        formatted_text = format_chat_ctx(chat_ctx[-MAX_HISTORY:])\n",
    "        inputs = turn_tokenizer(\n",
    "            formatted_text,\n",
    "            return_tensors=\"np\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_HISTORY_TOKENS,\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].astype(np.int64)\n",
    "        outputs = session.run([\"logits\"], {\"input_ids\": input_ids})\n",
    "        logits = outputs[0][0, -1, :]\n",
    "        probs = softmax(logits)\n",
    "        eou_token_id = turn_tokenizer.encode(\"<|im_end|>\")[0]\n",
    "        logger.debug(f\"Input text: {formatted_text}, EOU token ID: {eou_token_id}, Probs: {probs}\")\n",
    "        return probs[eou_token_id]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"EOU calculation error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "async def generate_speech_from_text(text: str) -> np.ndarray:\n",
    "    try:\n",
    "        prompt = \"\".join(\n",
    "            [\n",
    "                \"<|task_tts|>\",\n",
    "                \"<|start_content|>\",\n",
    "                text,\n",
    "                \"<|end_content|>\",\n",
    "                \"<|start_global_token|>\",\n",
    "            ]\n",
    "        )\n",
    "        model_inputs = spark_tokenizer([prompt], return_tensors=\"pt\").to(spark_model.device)\n",
    "        generated_ids = spark_model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.2,\n",
    "            do_sample=True,\n",
    "            pad_token_id=spark_tokenizer.pad_token_id,\n",
    "        )\n",
    "        generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
    "        predicts_text = spark_tokenizer.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=False\n",
    "        )[0]\n",
    "        semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicts_text)\n",
    "        if not semantic_matches:\n",
    "            logger.warning(\"No semantic matches found in TTS output\")\n",
    "            return np.array([], dtype=np.float32)\n",
    "        pred_semantic_ids = torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0)\n",
    "        global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicts_text)\n",
    "        pred_global_ids = (\n",
    "            torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0)\n",
    "            if global_matches\n",
    "            else torch.zeros((1, 1), dtype=torch.long)\n",
    "        )\n",
    "        pred_global_ids = pred_global_ids.unsqueeze(0)\n",
    "        wav_np = audio_tokenizer.detokenize(pred_global_ids.squeeze(0), pred_semantic_ids)\n",
    "        return wav_np\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Speech generation error: {e}\")\n",
    "        return np.array([], dtype=np.float32)\n",
    "\n",
    "class ConversationManager:\n",
    "    def __init__(self):\n",
    "        self.chat_history = []\n",
    "        self.sample_rate = 16000\n",
    "        self.is_processing = False\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.transcript_buffer = []\n",
    "\n",
    "    async def process_conversation(self, transcript: str) -> tuple:\n",
    "        async with self.lock:\n",
    "            if self.is_processing:\n",
    "                logger.info(\"Skipping transcript processing: already in progress\")\n",
    "                return transcript, 0.0, False, None, None\n",
    "            self.is_processing = True\n",
    "            try:\n",
    "                if not transcript.strip():\n",
    "                    logger.debug(\"Received empty transcript, skipping\")\n",
    "                    return transcript, 0.0, False, None, None\n",
    "                logger.debug(f\"Received transcript: {transcript}\")\n",
    "                self.transcript_buffer.append(transcript)\n",
    "                current_transcript = \" \".join(self.transcript_buffer)\n",
    "                messages = self.chat_history + [{\"role\": \"user\", \"content\": current_transcript}]\n",
    "                eou_prob = calculate_eou(messages, onnx_session)\n",
    "                logger.debug(f\"EOU score for '{current_transcript}': {eou_prob}\")\n",
    "                is_final = eou_prob >= EOU_THRESHOLD\n",
    "                if not is_final:\n",
    "                    logger.debug(f\"EOU score {eou_prob} below threshold {EOU_THRESHOLD}, buffering\")\n",
    "                    return current_transcript, eou_prob, False, None, None\n",
    "                logger.info(f\"Processing final transcript: {current_transcript}\")\n",
    "                inputs = qwen_tokenizer.apply_chat_template(\n",
    "                    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "                ).to(qwen_model.device)\n",
    "                outputs = qwen_model.generate(inputs, max_new_tokens=156)\n",
    "                generated_text = qwen_tokenizer.decode(\n",
    "                    outputs[0][inputs.shape[1]:], skip_special_tokens=True\n",
    "                )\n",
    "                logger.info(f\"Generated response: {generated_text}\")\n",
    "                self.chat_history.append({\"role\": \"user\", \"content\": current_transcript})\n",
    "                self.chat_history.append({\"role\": \"assistant\", \"content\": generated_text})\n",
    "                if len(self.chat_history) > MAX_HISTORY * 2:\n",
    "                    self.chat_history = self.chat_history[-MAX_HISTORY * 2:]\n",
    "                generated_waveform = await generate_speech_from_text(generated_text)\n",
    "                self.transcript_buffer = []\n",
    "                logger.debug(\"Cleared transcript buffer\")\n",
    "                return current_transcript, eou_prob, True, generated_text, generated_waveform\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Conversation processing error: {e}\")\n",
    "                return current_transcript, eou_prob, False, None, None\n",
    "            finally:\n",
    "                self.is_processing = False\n",
    "\n",
    "async def offer(websocket: WebSocket, waveform: np.ndarray):\n",
    "    pc = RTCPeerConnection()\n",
    "    pcs.add(pc)\n",
    "    track = AudioStreamTrackImpl(waveform)\n",
    "    pc.addTrack(track)\n",
    "    await pc.setLocalDescription(await pc.createOffer())\n",
    "    await websocket.send_json({\"sdp\": pc.localDescription.sdp, \"type\": pc.localDescription.type})\n",
    "    answer = json.loads(await websocket.receive_text())\n",
    "    await pc.setRemoteDescription(RTCSessionDescription(sdp=answer[\"sdp\"], type=answer[\"type\"]))\n",
    "    return pc\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Assistant WebSocket server is running. Connect via /ws/assistant\"}\n",
    "\n",
    "@app.get(\"/test\")\n",
    "async def test_page():\n",
    "    return \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head><title>Assistant WS Test</title></head>\n",
    "    <body>\n",
    "    <h1>Assistant WebSocket Test</h1>\n",
    "    <textarea id=\"transcript\" rows=\"4\" cols=\"50\"></textarea><br/>\n",
    "    <button onclick=\"sendTranscript()\">Send Transcript</button>\n",
    "    <pre id=\"output\"></pre>\n",
    "    <script>\n",
    "      const ws = new WebSocket(\"ws://\" + location.host + \"/ws/assistant\");\n",
    "      ws.onmessage = (event) => {\n",
    "        const data = JSON.parse(event.data);\n",
    "        document.getElementById('output').textContent += JSON.stringify(data, null, 2) + \"\\\\n\";\n",
    "      };\n",
    "      function sendTranscript() {\n",
    "        const transcript = document.getElementById('transcript').value;\n",
    "        ws.send(JSON.stringify({transcript: transcript}));\n",
    "      }\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    logger.info(\"Starting Assistant server...\")\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    logger.info(\"Shutting down Assistant server...\")\n",
    "\n",
    "@app.websocket(\"/ws/assistant\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    cm = ConversationManager()\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                data = json.loads(await websocket.receive_text())\n",
    "                if \"sdp\" in data and \"type\" in data:\n",
    "                    pc = RTCPeerConnection()\n",
    "                    pcs.add(pc)\n",
    "                    await pc.setRemoteDescription(RTCSessionDescription(sdp=data[\"sdp\"], type=data[\"type\"]))\n",
    "                    track = AudioStreamTrackImpl(np.array([]))  # Placeholder\n",
    "                    pc.addTrack(track)\n",
    "                    await pc.setLocalDescription(await pc.createAnswer())\n",
    "                    await websocket.send_json({\"sdp\": pc.localDescription.sdp, \"type\": pc.localDescription.type})\n",
    "                    continue\n",
    "                transcript = data.get(\"transcript\", \"\")\n",
    "                logger.info(f\"WebSocket input: transcript={transcript}\")\n",
    "                if not transcript.strip():\n",
    "                    continue\n",
    "                transcript, eou_score, complete, resp, waveform = await cm.process_conversation(transcript)\n",
    "                await websocket.send_json({\n",
    "                    \"type\": \"transcript\",\n",
    "                    \"transcript\": transcript,\n",
    "                    \"eou_score\": float(eou_score),\n",
    "                    \"is_complete\": complete,\n",
    "                })\n",
    "                if resp and waveform is not None:\n",
    "                    await asyncio.sleep(0.1)\n",
    "                    await websocket.send_json({\"type\": \"assistant_response\", \"response\": resp})\n",
    "                    await offer(websocket, waveform)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"WebSocket error: {e}\")\n",
    "                if websocket.client_state == WebSocketState.CONNECTED:\n",
    "                    await websocket.close(code=1000, reason=str(e))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"WebSocket connection closed with error: {e}\")\n",
    "    finally:\n",
    "        for pc in pcs:\n",
    "            await pc.close()\n",
    "        pcs.clear()\n",
    "\n",
    "# === Run with ngrok ===\n",
    "from pyngrok import ngrok\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"Assistant public URL:\", public_url)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import uvicorn\n",
    "from uvicorn.config import Config\n",
    "from uvicorn.server import Server\n",
    "\n",
    "config = Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "server = Server(config=config)\n",
    "await server.serve()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 356874,
     "modelInstanceId": 335866,
     "sourceId": 411371,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 357007,
     "modelInstanceId": 335999,
     "sourceId": 411526,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
